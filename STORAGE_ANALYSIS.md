# UVPFL Storage Analysis & Recommendations

## Current Storage Usage

### Project Directory (`/home/arunabha/Shini/UVPFL`)
| Item | Size | Description |
|------|------|-------------|
| `360dataset/` | **1.7 GB** | Original dataset (videos, orientation data) |
| `processed_data/` | **186 MB** | Preprocessed data (can be regenerated) |
| Code files | < 1 MB | Python scripts |
| **Total Project** | **~1.9 GB** | |

### System Directories
| Item | Size | Description |
|------|------|-------------|
| `~/.keras/models/` | **91 MB** | ResNet50 pre-trained weights (downloaded once) |
| `~/.cache/` | **10 GB** | System cache (pip, conda, etc.) |
| **Home Directory** | **30 GB** | Total |

---

## Future Storage Requirements

### During Training
| Item | Estimated Size | Notes |
|------|----------------|-------|
| Model checkpoints | 100-200 MB each | Save only best model |
| Training logs | 10-50 MB | TensorBoard logs |
| Saved models | ~100 MB each | Final trained models |
| **Total Additional** | **~500 MB - 1 GB** | |

### Maximum Estimated Total
- **Current**: ~1.9 GB (project) + 91 MB (weights) = **~2 GB**
- **After Training**: ~2 GB + 1 GB = **~3 GB maximum**

---

## Storage Optimization Recommendations

### ✅ Safe to Delete (Can Regenerate)

1. **`processed_data/` (186 MB)**
   - Can be regenerated by running `data_preprocessing.py`
   - Delete if needed: `rm -rf processed_data/`
   - Regenerate: `python data_preprocessing.py`

2. **`~/.cache/` (10 GB)** ⚠️ **LARGE**
   - Contains pip, conda, and other caches
   - Safe to clean: `pip cache purge` or `rm -rf ~/.cache/pip`
   - Can free up significant space

3. **Model Checkpoints (during training)**
   - Keep only the best model checkpoint
   - Delete intermediate checkpoints
   - Use `ModelCheckpoint` with `save_best_only=True`

### ✅ Keep (Essential)

1. **`360dataset/` (1.7 GB)** - Original dataset (required)
2. **`~/.keras/models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5` (91 MB)** - Pre-trained weights (downloaded once)

---

## Cleanup Commands

### Clean pip cache (can free several GB)
```bash
pip cache purge
```

### Clean conda cache (if using conda)
```bash
conda clean --all
```

### Remove processed data (can regenerate)
```bash
cd /home/arunabha/Shini/UVPFL
rm -rf processed_data/
# Regenerate later with: python data_preprocessing.py
```

### Clean old checkpoints (keep only best)
```bash
cd /home/arunabha/Shini/UVPFL
# Keep only best checkpoint, delete others
find checkpoints/ -name "*_latest.keras" -delete
```

---

## Storage Strategy for Training

1. **Use minimal checkpoints**: Save only best model
2. **Clean cache regularly**: Run `pip cache purge` periodically
3. **Delete old logs**: Keep only recent TensorBoard logs
4. **Monitor during training**: Check `du -sh checkpoints/ logs/` regularly

---

## Summary

- **Current project usage**: ~1.9 GB (reasonable)
- **Largest issue**: `~/.cache/` at 10 GB (can be cleaned)
- **Future needs**: +500 MB - 1 GB for training artifacts
- **Total maximum**: ~3 GB for complete project

**Recommendation**: Clean `~/.cache/` first to free up space, then proceed with training.


